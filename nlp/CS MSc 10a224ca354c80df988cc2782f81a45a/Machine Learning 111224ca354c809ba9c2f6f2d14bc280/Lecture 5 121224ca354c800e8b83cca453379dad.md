# Lecture 5

# K-Nearest Neighbours [#5 - 4:23]:

- unsupervised learning
- With labelled data in a feature space, select K closet neighbour to the new sample and label it with the **majority label in K.**
- Algo:
    
    ```jsx
    x as data_to_predict
    for t_node in t_training_set:
    	calculate Distance(x, t_node)
    k_neighs = select K from min([ Distance(x,t_node) ])
    return max(label for n in k_neighs) as x's label
    ```
    

 

Disadvantages:

- will not work on datasets that are not **normalised** (one feature’s range is larger than the others)
- computationally expensive to calculate distanced for every single new  input.

Hyper parameter:

- K - should use odd numbers

Advantages:

- can be used for **regression** and classification
- works for multi-class classification

# Support Vector Machines (SVM)

**Intuition:**

- SVM defined a margin around a decision boundary and tries to maximise the space in said margin

**“All you need to remember” - about Hard SVM**

- maximize margin - minimize ||w||
- && the decision boundary needs to separate the two classes
    
    ![Screenshot 2024-10-16 at 8.32.18 PM.png](Lecture%205%20121224ca354c800e8b83cca453379dad/Screenshot_2024-10-16_at_8.32.18_PM.png)
    

Advantages:

- works for regression and classification

Disadvantages:

- Works only with linear separable datasets
- because of the need to perform a **quadratic solver** equation, SVMs do not work well on datasets with a lot of points
    - requires a matrix or pair wise dot product between every point
    - **100 points in dataset will result in a 100 x 100 matrix**
        
        ![Screenshot 2024-10-16 at 8.27.52 PM.png](Lecture%205%20121224ca354c800e8b83cca453379dad/Screenshot_2024-10-16_at_8.27.52_PM.png)
        

(Math not tested)

- [Point-Plain Distance explaination](https://mathworld.wolfram.com/Point-PlaneDistance.html)

[Norm of a vector](Lecture%205%20121224ca354c800e8b83cca453379dad/Norm%20of%20a%20vector%20121224ca354c8079b7f3cbeed422bbc6.md)

- How to find the margin boarder:
    - use Larange Multiplier

**Hard Margin SVM (^ all the above)**

- this only works if a true line can be found that separates the two labels (**Linear separable).**

**Soft Margin SVM**

- Allow for some violations
    - **The Slack Term -** measures the violation for Xn
    
    ![Screenshot 2024-10-16 at 8.33.59 PM.png](Lecture%205%20121224ca354c800e8b83cca453379dad/Screenshot_2024-10-16_at_8.33.59_PM.png)
    
    - we add the sum of the slack value of each data point to the norm
- Hyper-parameter **C** to balance the terms like in the **Lasso Method**
    - when C increases the decision boundary decreases

# Kernel Support Vector Machine

NonLinear dataset

**Intuition:**

- Transform dataset into a higher dimension space where non-linear datasets become linear separable - then apply Soft SVM to solve
    
    ![Screenshot 2024-10-16 at 8.40.55 PM.png](Lecture%205%20121224ca354c800e8b83cca453379dad/Screenshot_2024-10-16_at_8.40.55_PM.png)
    

**Issues with intuition:**

- since SVM is already computationally expensive, transforming data into a higher dimensional space is gonna blow up your computer

**Solution:**

- “Kernel Trick” - choose a function to apply tho the two original features to map to a higher dimension while having the ability to calculate the dot product in higher dimensional space in the original dimensional space.
- **Quadratic Function** as Kernel
    1. original: {x1, x2}
    2. map to higher dimension with quad function: {x1, x2, x1^2, x2^2, x1x2}
    3. #? **“Play a trick”** to convert to: 
        
        ![Screenshot 2024-10-16 at 8.54.37 PM.png](Lecture%205%20121224ca354c800e8b83cca453379dad/Screenshot_2024-10-16_at_8.54.37_PM.png)
        
        - add vector of 1 to space
        - add sqrt of 2 to x1, x2, and x1x2
        - #? **leave x1^2 and x2^2** as is
            - *#? will this not affect the representation of the features? if we can just pick and choose?*
    4. The dot product of the dimensions in step 3 will result in this condensed form:
        
        ![Screenshot 2024-10-16 at 8.58.01 PM.png](Lecture%205%20121224ca354c800e8b83cca453379dad/Screenshot_2024-10-16_at_8.58.01_PM.png)
        
        - The right-hand side calculations can be done in the original dimensional space, which saves you a lot of computing power while allowing for elevation of your dataset.
- **Other Kernel Options:**
    - **Polynomial kernel** - like the quad function above but can be elevated to a higher power
        - hyper-param is ***m***
    - **Gaussian Kernel -** distance between pairs
        - hyper-param is ***sigma***
            - determines the shape of the Gaussian
        
        ![Screenshot 2024-10-16 at 9.02.09 PM.png](Lecture%205%20121224ca354c800e8b83cca453379dad/Screenshot_2024-10-16_at_9.02.09_PM.png)
        

# SVM for Regression

![Screenshot 2024-10-16 at 9.05.52 PM.png](Lecture%205%20121224ca354c800e8b83cca453379dad/Screenshot_2024-10-16_at_9.05.52_PM.png)