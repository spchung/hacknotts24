# Lecture 6 15/10/24

Review: **The Pre-processing pipeline**

![Screenshot 2024-10-16 at 10.48.48 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_10.48.48_PM.png)

## Overview:

**Representation**:

- All words in relevant documents can be represented in a vector
- A vocabulary of “I, love, cat, dog, pigeon” can be represented as a vector of length 5
    - The sentence “I love cat” then can be translated into [1,1,1,0,0] in vector space
- with representation you can find the closeness of each sentence within this vector space

**Similarity:**

- binary similarity, cosine similarity … etc

**Weighting:**

- so far in the vector space, we are just counting the num of occurrences but that might not be the best as issue of common an common words arise.
- Therefore, weighting function are introduced to give more meaningful representations of a word in the vector space.
- **Methods:**
    - binary weighting (if exists once then 1 else 0)
    - log frequency term weighting (log of term freq)
    - TF-IDF

**Word Embedding:**

- COBW - continuous bag of words

**Vector space in a Bag of Words**

- **n** being the number of **words in the vocab** and **number of dimensions**
- each word in the bad of word takes one dimension
    - [w1,w2,w2…w3]
- then we can produce a vector for any combination of words
- using each sents vector we can calculate how similar each sentence is
    
    ![Screenshot 2024-10-16 at 10.54.05 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_10.54.05_PM.png)
    

**How to measure similarity:**

- **Jaccard index**
    - perform **AND** operation on two vectors like in bitwise
- **Euclidean distance**
    
    ![Screenshot 2024-10-16 at 11.00.59 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_11.00.59_PM.png)
    
    ![Screenshot 2024-10-16 at 11.02.18 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_11.02.18_PM.png)
    
    - gotta make sure the denominator is never 0
    - represents **distance between points**
- **cosine similarity**
    
    ![Screenshot 2024-10-16 at 11.04.39 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_11.04.39_PM.png)
    
    - this is better than **Euclidean distance because**
        - Some comparisons will be made between different length of document, where D1 might mention Word1 99 times and D2 only once. With **Euclidean distance,** the distance will be massive just because D1 repeats the word so much.
        - Cosine similarity calculates the **difference in angles** in two vectors, which neglects the effects of having a very distant point in the vector space.

### **Drawbacks of using word frequency to represent vector space**

- You end up putting things very far apart that actually might be similar in meaning.

### Weighting Methods:

- Binary Weighting
    - If something appears once than it is 1 else 0
- Log-frequency term weighting
    - log the frequency nums - gives us a flatter value distribution
        
        ![Screenshot 2024-10-16 at 11.16.36 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_11.16.36_PM.png)
        
    - Add one to frequency becasue log(1) is 0
- **TF-IDF term weighting**
    
    ![Screenshot 2024-10-16 at 11.17.05 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_11.17.05_PM.png)
    
    - log frequency * **log(Num of Docs / num of documents containing that term)**
        
        ![Screenshot 2024-10-16 at 11.19.33 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_11.19.33_PM.png)
        
    - if a word exists in all documents N/n will be 1 ad log(1) will be 0 giving this term no weight
    - for a word that only exist N/(N/2) times this term will have a lot more weight hence making the existence of this more more meaningful

# One-hot encoding

- represent in matrix form
- **Assumption:**
    - all words are equally unrelated
    - no semantics
    - large **sparse** matrices

# Word Embedding

- **Distributional** semantics - modelling meaning based on the distribution of languages in large samples of data.
    - ‘cats’ appear in similar context as ‘dog’ therefore more closely related than ‘bee’ … etc
    - ‘red’ and ‘blue’ rather than ‘large’
- **Dense** vector representation of words.
    - Similarity
    - Analogy

# Word2Vec - CBOW and SkipGram

**CBOW - continuous bag of words model**

![Screenshot 2024-10-16 at 11.38.42 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_11.38.42_PM.png)

- Initialise a fully connected, std neural network
- Task of the network:
    - travel through a large doc word by word
    - train the network to predict words based on their context
- e.g. use “that no ___ has accompanies” to predict the outcome “disaster”

**SkipGram**

![Screenshot 2024-10-16 at 11.39.03 PM.png](Lecture%206%2015%2010%2024%20120224ca354c8017bdf0c9978dbbc5fa/Screenshot_2024-10-16_at_11.39.03_PM.png)

- Tries to predict the context from the pivot word (opposite of CBOW)
- word embedding are extracted from the weights of the network

### Drawbacks of using context to infer word’s embedding vectors

- Preservation of societal biases
- **No true semantic understanding**
    - ‘cats’ are closer to ‘dogs’ than ‘cats’ are to ‘tigers’

# Search as Similarity

- word embedding represents similarity, similarity can get you close to any query

# Classification as similarity with K-NN

K-NN classifier

- given a corpus of labeled documents
- given a new document *d* to label
- look at K documents **most similar** to *d* from corpus

Usually with a Bag-of-Words representation

Challenges: choosing **K**

- for classification - 3-NN or 5-NN
- for retrieval - 1-NN

# Advanced: Retrieval Augumented Generation (RAG)